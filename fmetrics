#!/usr/bin/env bash
# fmetrics — performance telemetry and analytics for fsuite tools.
# Part of fsuite. Provides stats, history, predictions, and data management.

set -euo pipefail

VERSION="1.6.0"

# -------------------------
# Paths
# -------------------------
FSUITE_DIR="$HOME/.fsuite"
JSONL_FILE="$FSUITE_DIR/telemetry.jsonl"
DB_FILE="$FSUITE_DIR/telemetry.db"
SCRIPT_DIR="$(cd "$(dirname "$0")" && pwd)"

# -------------------------
# Defaults
# -------------------------
DEFAULT_OUTPUT="pretty"
DEFAULT_LIMIT=20
DEFAULT_CLEAN_DAYS=90

# -------------------------
# Helpers
# -------------------------
die() {
  local code=1
  if [[ "${1:-}" =~ ^[0-9]+$ ]]; then code="$1"; shift; fi
  echo "fmetrics: $*" >&2
  exit "$code"
}

has() { command -v "$1" >/dev/null 2>&1; }

json_escape() {
  sed -e 's/\\/\\\\/g' -e 's/"/\\"/g' -e 's/\t/\\t/g' -e 's/\r/\\r/g' -e ':a;N;$!ba;s/\n/\\n/g'
}

human_ms() {
  local ms="$1"
  if (( ms >= 60000 )); then
    awk "BEGIN {printf \"%.1fm\", $ms/60000}"
  elif (( ms >= 1000 )); then
    awk "BEGIN {printf \"%.1fs\", $ms/1000}"
  else
    echo "${ms}ms"
  fi
}

human_bytes() {
  local bytes="$1"
  if (( bytes < 0 )); then
    echo "?"
  elif (( bytes < 1024 )); then
    echo "${bytes}B"
  elif (( bytes < 1048576 )); then
    awk "BEGIN {printf \"%.1fK\", $bytes/1024}"
  elif (( bytes < 1073741824 )); then
    awk "BEGIN {printf \"%.1fM\", $bytes/1048576}"
  else
    awk "BEGIN {printf \"%.1fG\", $bytes/1073741824}"
  fi
}

# SQL escape helper - prevents injection, handles special chars consistently
sql_escape() {
  local val="$1"
  # Strip unsafe characters, keep alphanumeric and safe punctuation
  val=$(printf '%s' "$val" | tr -cd '[:alnum:]. _:-')
  # Escape single quotes for SQL
  val="${val//\'/\'\'}"
  printf '%s' "$val"
}

_find_predict_script() {
  local candidates=(
    "$SCRIPT_DIR/fmetrics-predict.py"
    "$SCRIPT_DIR/fmetrics-predict"
    "/usr/share/fsuite/fmetrics-predict.py"
    "/usr/lib/fsuite/fmetrics-predict.py"
  )
  for c in "${candidates[@]}"; do
    [[ -f "$c" ]] && { printf "%s" "$c"; return 0; }
  done
  return 1
}

# Portable JSON field extraction (no grep -oP / PCRE dependency)
# Handles both "key":"stringval" and "key":numval
json_get() {
  local line="$1" key="$2"
  echo "$line" | sed -n 's/.*"'"$key"'"[[:space:]]*:[[:space:]]*"\{0,1\}\([^",}]*\)"\{0,1\}.*/\1/p'
}

ensure_db() {
  has sqlite3 || die 3 "sqlite3 is required. Run: fmetrics --install-hints"
  mkdir -p "$FSUITE_DIR" 2>/dev/null || die "Cannot create $FSUITE_DIR"
  sqlite3 "$DB_FILE" <<'SQL' >/dev/null 2>&1
CREATE TABLE IF NOT EXISTS telemetry (
  id INTEGER PRIMARY KEY AUTOINCREMENT,
  timestamp TEXT NOT NULL,
  tool TEXT NOT NULL,
  version TEXT NOT NULL,
  mode TEXT NOT NULL,
  path_hash TEXT NOT NULL,
  project_name TEXT NOT NULL,
  duration_ms INTEGER NOT NULL,
  exit_code INTEGER NOT NULL,
  depth INTEGER NOT NULL DEFAULT -1,
  items_scanned INTEGER NOT NULL DEFAULT -1,
  bytes_scanned INTEGER NOT NULL DEFAULT -1,
  flags TEXT NOT NULL DEFAULT '',
  backend TEXT NOT NULL DEFAULT '',
  UNIQUE(timestamp, tool, path_hash)
);
CREATE INDEX IF NOT EXISTS idx_tool ON telemetry(tool);
CREATE INDEX IF NOT EXISTS idx_project ON telemetry(project_name);
CREATE INDEX IF NOT EXISTS idx_timestamp ON telemetry(timestamp);
PRAGMA journal_mode=WAL;
SQL

  # Schema migration for Tier 2 hardware columns (idempotent - errors suppressed)
  sqlite3 "$DB_FILE" "ALTER TABLE telemetry ADD COLUMN cpu_temp_mc INTEGER DEFAULT -1;" 2>/dev/null || true
  sqlite3 "$DB_FILE" "ALTER TABLE telemetry ADD COLUMN disk_temp_mc INTEGER DEFAULT -1;" 2>/dev/null || true
  sqlite3 "$DB_FILE" "ALTER TABLE telemetry ADD COLUMN ram_total_kb INTEGER DEFAULT -1;" 2>/dev/null || true
  sqlite3 "$DB_FILE" "ALTER TABLE telemetry ADD COLUMN ram_available_kb INTEGER DEFAULT -1;" 2>/dev/null || true
  sqlite3 "$DB_FILE" "ALTER TABLE telemetry ADD COLUMN load_avg_1m TEXT DEFAULT '-1';" 2>/dev/null || true
  sqlite3 "$DB_FILE" "ALTER TABLE telemetry ADD COLUMN filesystem_type TEXT DEFAULT 'unknown';" 2>/dev/null || true
  sqlite3 "$DB_FILE" "ALTER TABLE telemetry ADD COLUMN storage_type TEXT DEFAULT 'unknown';" 2>/dev/null || true
}

# -------------------------
# Usage
# -------------------------
usage() {
  cat <<'EOF'
fmetrics — performance telemetry and analytics for fsuite

USAGE
  fmetrics <subcommand> [options]

SUBCOMMANDS
  stats              Show dashboard of tool usage, runtimes, reliability
  history            Show recent runs (filterable)
  predict <path>     Estimate how long fsuite tools will take on <path>
  import             Ingest telemetry.jsonl into SQLite database
  clean              Prune old telemetry data
  profile            Show machine profile (Tier 3 telemetry)

OPTIONS (global)
  -o, --output       Output format: pretty (default) or json
  -h, --help         Show this help
  --version          Print version
  --self-check       Verify dependencies
  --install-hints    Print install commands for missing dependencies

OPTIONS (history)
  --tool <name>      Filter by tool (ftree, fsearch, fcontent)
  --project <name>   Filter by project name
  --limit <N>        Max rows (default 20)

OPTIONS (predict)
  --tool <name>      Predict for specific tool only (ftree, fsearch, fcontent)

OPTIONS (clean)
  --days <N>         Keep last N days (default 90)
  --dry-run          Preview what would be deleted

ENVIRONMENT
  FSUITE_TELEMETRY=0  Disable telemetry collection in fsuite tools

EXAMPLES
  fmetrics import              Import telemetry data into SQLite
  fmetrics stats               Show usage dashboard
  fmetrics stats -o json       Machine-readable stats
  fmetrics history --tool ftree --limit 10
  fmetrics predict /project    Estimate runtimes for /project
  fmetrics clean --days 30     Remove data older than 30 days
EOF
}

# -------------------------
# Subcommand: import
# -------------------------
cmd_import() {
  ensure_db

  if [[ ! -f "$JSONL_FILE" ]]; then
    echo "No telemetry data found at $JSONL_FILE"
    echo "Run fsuite tools (ftree, fsearch, fcontent) to generate telemetry."
    return 0
  fi

  local total=0 inserted=0 skipped=0 errors=0

  while IFS= read -r line; do
    [[ -z "$line" ]] && continue
    (( total++ )) || true

    # Parse JSON fields using portable json_get (no grep -oP / PCRE dependency)
    local ts tool ver mode ph pn dur ec dep items bytes flags backend
    local cpu_temp disk_temp ram_total ram_avail load_avg
    ts=$(json_get "$line" "timestamp") || true
    [[ -n "$ts" ]] || { (( errors++ )); continue; }
    tool=$(json_get "$line" "tool") || true
    [[ -n "$tool" ]] || { (( errors++ )); continue; }
    ver=$(json_get "$line" "version") || ver=""
    mode=$(json_get "$line" "mode") || mode=""
    ph=$(json_get "$line" "path_hash") || ph=""
    pn=$(json_get "$line" "project_name") || pn=""
    dur=$(json_get "$line" "duration_ms") || dur=0
    [[ "$dur" =~ ^-?[0-9]+$ ]] || dur=0
    ec=$(json_get "$line" "exit_code") || ec=0
    [[ "$ec" =~ ^[0-9]+$ ]] || ec=0
    dep=$(json_get "$line" "depth") || dep=-1
    [[ "$dep" =~ ^-?[0-9]+$ ]] || dep=-1
    items=$(json_get "$line" "items_scanned") || items=-1
    [[ "$items" =~ ^-?[0-9]+$ ]] || items=-1
    bytes=$(json_get "$line" "bytes_scanned") || bytes=-1
    [[ "$bytes" =~ ^-?[0-9]+$ ]] || bytes=-1
    flags=$(json_get "$line" "flags") || flags=""
    backend=$(json_get "$line" "backend") || backend=""
    # Hardware telemetry fields (Tier 2)
    cpu_temp=$(json_get "$line" "cpu_temp_mc") || cpu_temp=-1
    [[ "$cpu_temp" =~ ^-?[0-9]+$ ]] || cpu_temp=-1
    disk_temp=$(json_get "$line" "disk_temp_mc") || disk_temp=-1
    [[ "$disk_temp" =~ ^-?[0-9]+$ ]] || disk_temp=-1
    ram_total=$(json_get "$line" "ram_total_kb") || ram_total=-1
    [[ "$ram_total" =~ ^-?[0-9]+$ ]] || ram_total=-1
    ram_avail=$(json_get "$line" "ram_available_kb") || ram_avail=-1
    [[ "$ram_avail" =~ ^-?[0-9]+$ ]] || ram_avail=-1
    load_avg=$(json_get "$line" "load_avg_1m") || load_avg="-1"
    [[ "$load_avg" =~ ^[0-9.]+$ ]] || load_avg="-1"
    fs_type=$(json_get "$line" "filesystem_type") || fs_type="unknown"
    [[ -z "$fs_type" ]] && fs_type="unknown"
    stor_type=$(json_get "$line" "storage_type") || stor_type="unknown"
    [[ -z "$stor_type" ]] && stor_type="unknown"

    # Sanitize string fields using helper (strips unsafe chars + escapes quotes)
    ts=$(sql_escape "$ts")
    tool=$(sql_escape "$tool")
    ver=$(sql_escape "$ver")
    mode=$(sql_escape "$mode")
    ph=$(sql_escape "$ph")
    pn=$(sql_escape "$pn")
    flags=$(sql_escape "$flags")
    backend=$(sql_escape "$backend")
    load_avg=$(sql_escape "$load_avg")
    fs_type=$(sql_escape "$fs_type")
    stor_type=$(sql_escape "$stor_type")

    local result
    result=$(sqlite3 "$DB_FILE" "INSERT OR IGNORE INTO telemetry (timestamp,tool,version,mode,path_hash,project_name,duration_ms,exit_code,depth,items_scanned,bytes_scanned,flags,backend,cpu_temp_mc,disk_temp_mc,ram_total_kb,ram_available_kb,load_avg_1m,filesystem_type,storage_type) VALUES ('$ts','$tool','$ver','$mode','$ph','$pn',$dur,$ec,$dep,$items,$bytes,'$flags','$backend',$cpu_temp,$disk_temp,$ram_total,$ram_avail,'$load_avg','$fs_type','$stor_type'); SELECT changes();" 2>/dev/null) || { (( errors++ )); continue; }

    if [[ "$result" == "1" ]]; then
      (( inserted++ )) || true
    else
      (( skipped++ )) || true
    fi
  done < "$JSONL_FILE"

  if [[ "$OUTPUT" == "json" ]]; then
    echo "{\"tool\":\"fmetrics\",\"version\":\"$VERSION\",\"subcommand\":\"import\",\"total_lines\":$total,\"inserted\":$inserted,\"skipped\":$skipped,\"errors\":$errors,\"db_path\":\"$DB_FILE\"}"
  else
    echo "Importing telemetry data..."
    echo "  Processed $total lines"
    echo "  Inserted $inserted new records"
    echo "  Skipped $skipped duplicates"
    (( errors > 0 )) && echo "  Errors: $errors (malformed lines)"
    local db_size
    db_size=$(du -h "$DB_FILE" 2>/dev/null | cut -f1) || db_size="unknown"
    echo "Database: $DB_FILE ($db_size)"
  fi
}

# -------------------------
# Subcommand: stats
# -------------------------
cmd_stats() {
  ensure_db

  local count
  count=$(sqlite3 "$DB_FILE" "SELECT COUNT(*) FROM telemetry;" 2>/dev/null) || count=0

  if (( count == 0 )); then
    echo "No telemetry data in database."
    echo "Run: fmetrics import"
    return 0
  fi

  local oldest newest db_size
  oldest=$(sqlite3 "$DB_FILE" "SELECT MIN(timestamp) FROM telemetry;")
  newest=$(sqlite3 "$DB_FILE" "SELECT MAX(timestamp) FROM telemetry;")

  # Compatible SQL: avg, min, max, success rate per tool
  local stats_sql
  stats_sql="SELECT tool, COUNT(*), CAST(AVG(duration_ms) AS INTEGER), MIN(duration_ms), MAX(duration_ms), ROUND(100.0 * SUM(CASE WHEN exit_code=0 THEN 1 ELSE 0 END) / COUNT(*), 1) FROM telemetry GROUP BY tool ORDER BY tool;"
  local projects_sql="SELECT project_name, COUNT(*) as c FROM telemetry GROUP BY project_name ORDER BY c DESC LIMIT 5;"

  if [[ "$OUTPUT" == "json" ]]; then
    db_size=$(stat -c%s "$DB_FILE" 2>/dev/null) || db_size=0
    echo -n "{\"tool\":\"fmetrics\",\"version\":\"$VERSION\",\"subcommand\":\"stats\","
    echo -n "\"total_runs\":$count,\"db_path\":\"$DB_FILE\",\"db_size_bytes\":$db_size,"
    echo -n "\"oldest_run\":\"$oldest\",\"newest_run\":\"$newest\","

    echo -n "\"tools\":["
    local first=1
    while IFS='|' read -r t runs avg_ms min_ms max_ms sr; do
      (( first )) || echo -n ","
      first=0
      echo -n "{\"name\":\"$t\",\"runs\":$runs,\"avg_ms\":$avg_ms,\"min_ms\":$min_ms,\"max_ms\":$max_ms,\"success_rate\":$sr}"
    done < <(sqlite3 -separator '|' "$DB_FILE" "$stats_sql" 2>/dev/null)
    echo -n "],"

    echo -n "\"top_projects\":["
    first=1
    while IFS='|' read -r pname pruns; do
      (( first )) || echo -n ","
      first=0
      echo -n "{\"name\":\"$pname\",\"runs\":$pruns}"
    done < <(sqlite3 -separator '|' "$DB_FILE" "$projects_sql" 2>/dev/null)
    echo "]}"
  else
    echo "fmetrics stats"
    echo "============================================"
    echo ""
    printf "  %-12s %6s %10s %10s %10s %9s\n" "Tool" "Runs" "Avg" "Min" "Max" "Success"
    echo "  ----------------------------------------------------------"

    while IFS='|' read -r t runs avg_ms min_ms max_ms sr; do
      printf "  %-12s %6s %10s %10s %10s %8s%%\n" \
        "$t" "$runs" "$(human_ms "$avg_ms")" "$(human_ms "$min_ms")" "$(human_ms "$max_ms")" "$sr"
    done < <(sqlite3 -separator '|' "$DB_FILE" "$stats_sql" 2>/dev/null)

    echo ""
    echo "  Top Projects:"
    while IFS='|' read -r pname pruns; do
      printf "    %-30s %d runs\n" "$pname" "$pruns"
    done < <(sqlite3 -separator '|' "$DB_FILE" "$projects_sql" 2>/dev/null)

    db_size=$(du -h "$DB_FILE" 2>/dev/null | cut -f1) || db_size="?"
    echo ""
    echo "  Telemetry: $count total runs -- $oldest to $newest"
    echo "  Database: $DB_FILE -- $db_size"
  fi
}

# -------------------------
# Subcommand: history
# -------------------------
cmd_history() {
  ensure_db

  # Sanitize filter inputs to prevent SQL injection
  local safe_tool safe_project
  safe_tool=$(echo "${FILTER_TOOL:-}" | tr -cd '[:alnum:]._-')
  safe_project=$(echo "${FILTER_PROJECT:-}" | tr -cd '[:alnum:]. _-')
  safe_project="${safe_project//\'/\'\'}"

  local where_clauses=()
  [[ -n "$safe_tool" ]] && where_clauses+=("tool='$safe_tool'")
  [[ -n "$safe_project" ]] && where_clauses+=("project_name='$safe_project'")

  local where=""
  if (( ${#where_clauses[@]} > 0 )); then
    local IFS=" AND "
    where="WHERE ${where_clauses[*]}"
  fi

  if [[ "$OUTPUT" == "json" ]]; then
    echo -n "{\"tool\":\"fmetrics\",\"version\":\"$VERSION\",\"subcommand\":\"history\",\"runs\":["
    local first=1
    while IFS='|' read -r ts t mode pn dur ec items bytes backend; do
      (( first )) || echo -n ","
      first=0
      echo -n "{\"timestamp\":\"$ts\",\"tool\":\"$t\",\"mode\":\"$mode\",\"project\":\"$pn\",\"duration_ms\":$dur,\"exit_code\":$ec,\"items_scanned\":$items,\"bytes_scanned\":$bytes,\"backend\":\"$backend\"}"
    done < <(sqlite3 -separator '|' "$DB_FILE" "SELECT timestamp,tool,mode,project_name,duration_ms,exit_code,items_scanned,bytes_scanned,backend FROM telemetry $where ORDER BY timestamp DESC LIMIT $LIMIT;" 2>/dev/null)
    echo "]}"
  else
    local title="Recent Runs"
    [[ -n "${FILTER_TOOL:-}" ]] && title="Recent $FILTER_TOOL Runs"
    [[ -n "${FILTER_PROJECT:-}" ]] && title="$title (project: $FILTER_PROJECT)"
    echo "fmetrics history — $title (last $LIMIT)"
    echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
    printf "  %-20s %-12s %-12s %-12s %8s %6s %8s %6s\n" "Timestamp" "Tool" "Mode" "Project" "Time" "Items" "Bytes" "Exit"
    echo "  ─────────────────────────────────────────────────────────────────────────────────────────────"

    while IFS='|' read -r ts t mode pn dur ec items bytes backend; do
      printf "  %-20s %-12s %-12s %-12s %8s %6s %8s %6s\n" \
        "$ts" "$t" "$mode" "${pn:0:12}" "$(human_ms "$dur")" "$items" "$(human_bytes "$bytes")" "$ec"
    done < <(sqlite3 -separator '|' "$DB_FILE" "SELECT timestamp,tool,mode,project_name,duration_ms,exit_code,items_scanned,bytes_scanned,backend FROM telemetry $where ORDER BY timestamp DESC LIMIT $LIMIT;" 2>/dev/null)
  fi
}

# -------------------------
# Subcommand: predict
# -------------------------
_predict_size_mb() {
  # Compute MB from bytes, avoiding inline awk quoting issues
  local b="${1:-0}"
  if (( b > 0 )); then
    echo "$b" | awk '{printf "%.1f", $1/1048576}'
  else
    echo "0.0"
  fi
}

_predict_get_avg() {
  # Get average duration for a tool from DB
  local tool_name="$1"
  # Sanitize tool_name: escape single quotes for SQL safety
  local safe_tool="${tool_name//\'/\'\'}"
  local sql="SELECT CAST(AVG(duration_ms) AS INTEGER) FROM telemetry WHERE tool='${safe_tool}' AND exit_code=0;"
  local result
  result=$(sqlite3 "$DB_FILE" "$sql" 2>/dev/null) || result=0
  [[ -z "$result" ]] && result=0
  echo "$result"
}

cmd_predict() {
  local target="${PREDICT_PATH:-.}"
  [[ -d "$target" ]] || die "Not a directory: $target"

  if [[ -n "${PREDICT_TOOL:-}" ]]; then
    case "$PREDICT_TOOL" in
      ftree|fsearch|fcontent) ;;
      *) die "Invalid --tool for predict: $PREDICT_TOOL (use ftree, fsearch, or fcontent)" ;;
    esac
  fi

  ensure_db

  local count
  count=$(sqlite3 "$DB_FILE" "SELECT COUNT(*) FROM telemetry WHERE exit_code=0;" 2>/dev/null) || count=0

  # Quick recon scan to get project features
  local recon_json
  recon_json=$(FSUITE_TELEMETRY=0 "$SCRIPT_DIR/ftree" --recon -o json "$target" 2>/dev/null) || {
    die "Failed to run ftree --recon on $target"
  }

  local total_items total_bytes size_mb
  total_items=$(json_get "$recon_json" "total_entries") || total_items=0
  [[ "$total_items" =~ ^[0-9]+$ ]] || total_items=0
  total_bytes=$(json_get "$recon_json" "total_size_bytes") || total_bytes=0
  [[ "$total_bytes" =~ ^[0-9]+$ ]] || total_bytes=0
  size_mb=$(_predict_size_mb "$total_bytes")

  if (( count < 5 )); then
    if [[ "$OUTPUT" == "json" ]]; then
      echo -n "{\"tool\":\"fmetrics\",\"version\":\"$VERSION\",\"subcommand\":\"predict\","
      echo -n "\"path\":\"$target\",\"features\":{\"items\":$total_items,\"bytes\":$total_bytes},"
      echo "\"error\":\"insufficient_data\",\"total_samples\":$count,\"minimum_required\":5}"
    else
      echo "fmetrics predict -- $target"
      echo ""
      echo "  Project: $total_items entries, ${size_mb}MB"
      echo ""
      echo "  Insufficient telemetry data: $count runs, need at least 5."
      echo "  Run fsuite tools more to build prediction history."
      echo "  Then: fmetrics import && fmetrics predict $target"
    fi
    return 0
  fi

  # Check for Python predict helper
  local predict_py
  predict_py=$(_find_predict_script) || predict_py=""
  if [[ -n "$predict_py" ]] && has python3; then
    local _tool_arg=""
    [[ -n "${PREDICT_TOOL:-}" ]] && _tool_arg="--tool $PREDICT_TOOL"
    local pred_json
    pred_json=$(python3 "$predict_py" --db "$DB_FILE" --items "$total_items" --bytes "$total_bytes" --depth 3 --output json $_tool_arg 2>/dev/null) || pred_json=""

    if [[ -n "$pred_json" ]] && ! echo "$pred_json" | grep -q '"error"' 2>/dev/null; then
      if [[ "$OUTPUT" == "json" ]]; then
        echo "$pred_json"
      else
        echo "fmetrics predict -- $target"
        echo "============================================"
        echo ""
        echo "  Project Characteristics:"
        echo "    Items: $total_items"
        echo "    Size:  ${size_mb}MB"
        echo ""
        echo "  Estimated Runtimes:"
        local t pred_ms std_ms conf h_pred h_std tool_block
        local _predict_tools=(ftree fsearch fcontent)
        [[ -n "${PREDICT_TOOL:-}" ]] && _predict_tools=("$PREDICT_TOOL")
        for t in "${_predict_tools[@]}"; do
          # Extract the JSON block for this tool, then parse fields portably
          tool_block=$(echo "$pred_json" | sed -n 's/.*\({"tool"[^}]*"'"$t"'"[^}]*}\).*/\1/p' | head -1) || continue
          [[ -n "$tool_block" ]] || continue
          pred_ms=$(json_get "$tool_block" "predicted_ms") || continue
          [[ "$pred_ms" =~ ^-?[0-9]+$ ]] || continue
          (( pred_ms < 0 )) && continue
          std_ms=$(json_get "$tool_block" "std_dev_ms") || std_ms=0
          [[ "$std_ms" =~ ^[0-9]+$ ]] || std_ms=0
          conf=$(json_get "$tool_block" "confidence") || conf="?"
          h_pred=$(human_ms "$pred_ms")
          h_std=$(human_ms "$std_ms")
          printf '    %-12s ~%-8s +/-%s  [%s]\n' "$t" "$h_pred" "$h_std" "$conf"
        done
        local total_samples
        total_samples=$(json_get "$pred_json" "total_historical_samples") || total_samples="?"
        [[ "$total_samples" =~ ^[0-9]+$ ]] || total_samples="?"
        echo ""
        echo "  Based on $total_samples historical runs"
      fi
      return 0
    fi
  fi

  # Fallback: simple average-based prediction
  if [[ "$OUTPUT" == "json" ]]; then
    echo -n "{\"tool\":\"fmetrics\",\"version\":\"$VERSION\",\"subcommand\":\"predict\","
    echo -n "\"path\":\"$target\","
    echo -n "\"features\":{\"items\":$total_items,\"bytes\":$total_bytes},"
    echo -n "\"predictions\":["
    local first=1 t avg_ms
    local _predict_tools=(ftree fsearch fcontent)
    [[ -n "${PREDICT_TOOL:-}" ]] && _predict_tools=("$PREDICT_TOOL")
    for t in "${_predict_tools[@]}"; do
      (( first )) || echo -n ","
      first=0
      avg_ms=$(_predict_get_avg "$t")
      echo -n "{\"tool\":\"$t\",\"predicted_ms\":$avg_ms,\"method\":\"average\",\"confidence\":\"low\"}"
    done
    echo "],\"total_historical_samples\":$count,\"method\":\"average_fallback\"}"
  else
    echo "fmetrics predict -- $target"
    echo "============================================"
    echo ""
    echo "  Project: $total_items entries, ${size_mb}MB"
    echo ""
    echo "  Estimated Runtimes - based on historical averages:"
    local t avg_ms
    local _predict_tools=(ftree fsearch fcontent)
    [[ -n "${PREDICT_TOOL:-}" ]] && _predict_tools=("$PREDICT_TOOL")
    for t in "${_predict_tools[@]}"; do
      avg_ms=$(_predict_get_avg "$t")
      printf "    %-12s ~%s\n" "$t" "$(human_ms "$avg_ms")"
    done
    if ! has python3; then
      echo ""
      echo "  Note: Install python3 for k-NN predictions (more accurate)."
    elif ! _find_predict_script >/dev/null 2>&1; then
      echo ""
      echo "  Note: fmetrics-predict.py not found. Reinstall fsuite or check PATH."
    fi
    echo ""
    echo "  Based on $count historical runs"
  fi
}

# -------------------------
# Subcommand: clean
# -------------------------
cmd_clean() {
  ensure_db

  local days="${CLEAN_DAYS:-$DEFAULT_CLEAN_DAYS}"
  local cutoff
  cutoff=$(date -u -d "-${days} days" +%Y-%m-%dT%H:%M:%SZ 2>/dev/null) || \
    cutoff=$(date -u -v-"${days}d" +%Y-%m-%dT%H:%M:%SZ 2>/dev/null) || \
    die "Cannot compute date offset"

  local to_delete
  to_delete=$(sqlite3 "$DB_FILE" "SELECT COUNT(*) FROM telemetry WHERE timestamp < '$cutoff';" 2>/dev/null) || to_delete=0
  local to_keep
  to_keep=$(sqlite3 "$DB_FILE" "SELECT COUNT(*) FROM telemetry WHERE timestamp >= '$cutoff';" 2>/dev/null) || to_keep=0

  if (( DRY_RUN )); then
    if [[ "$OUTPUT" == "json" ]]; then
      echo "{\"tool\":\"fmetrics\",\"version\":\"$VERSION\",\"subcommand\":\"clean\",\"dry_run\":true,\"days\":$days,\"cutoff\":\"$cutoff\",\"would_delete\":$to_delete,\"would_keep\":$to_keep}"
    else
      echo "fmetrics clean (dry run)"
      echo "  Would delete $to_delete records older than $days days"
      echo "  Would keep $to_keep records"
    fi
    return 0
  fi

  if (( to_delete == 0 )); then
    echo "Nothing to clean. All $to_keep records are within the last $days days."
    return 0
  fi

  sqlite3 "$DB_FILE" "DELETE FROM telemetry WHERE timestamp < '$cutoff';"
  sqlite3 "$DB_FILE" "VACUUM;"

  if [[ "$OUTPUT" == "json" ]]; then
    local db_size
    db_size=$(stat -c%s "$DB_FILE" 2>/dev/null) || db_size=0
    echo "{\"tool\":\"fmetrics\",\"version\":\"$VERSION\",\"subcommand\":\"clean\",\"deleted\":$to_delete,\"kept\":$to_keep,\"days\":$days,\"db_size_bytes\":$db_size}"
  else
    local db_size
    db_size=$(du -h "$DB_FILE" 2>/dev/null | cut -f1) || db_size="?"
    echo "Cleaning telemetry older than $days days..."
    echo "  Deleted $to_delete records"
    echo "  Kept $to_keep records"
    echo "  Database: $DB_FILE ($db_size)"
  fi
}

# -------------------------
# Subcommand: profile
# -------------------------
cmd_profile() {
  local profile_file="$FSUITE_DIR/machine_profile.json"

  if [[ ! -f "$profile_file" ]]; then
    if [[ "$OUTPUT" == "json" ]]; then
      echo "{\"tool\":\"fmetrics\",\"version\":\"$VERSION\",\"subcommand\":\"profile\",\"error\":\"no profile\",\"hint\":\"Run any fsuite tool with FSUITE_TELEMETRY=3 to generate\"}"
    else
      echo "No machine profile found."
      echo "Generate one by running any fsuite tool with FSUITE_TELEMETRY=3"
      echo "Example: FSUITE_TELEMETRY=3 ftree ."
    fi
    return 0
  fi

  if [[ "$OUTPUT" == "json" ]]; then
    cat "$profile_file"
  else
    echo "Machine Profile ($profile_file)"
    echo "================================"
    cat "$profile_file" | sed 's/[{}",]//g' | sed 's/^ */  /' | grep -v '^$'
  fi
}

# -------------------------
# Self-check
# -------------------------
self_check() {
  echo "fmetrics dependency check:"
  if has sqlite3; then
    echo "  sqlite3: $(sqlite3 --version 2>&1 | head -1)"
  else
    echo "  sqlite3: NOT FOUND"
  fi
  if has python3; then
    echo "  python3: $(python3 --version 2>&1)"
  else
    echo "  python3: NOT FOUND (predictions unavailable, basic stats still work)"
  fi
  local predict_path
  predict_path=$(_find_predict_script 2>/dev/null) || predict_path=""
  if [[ -n "$predict_path" ]]; then
    echo "  fmetrics-predict.py: found at $predict_path"
  else
    echo "  fmetrics-predict.py: NOT FOUND (check installation path)"
  fi
  if has python3 && [[ -n "$predict_path" ]]; then
    echo "  k-NN predictions: available"
  else
    echo "  k-NN predictions: unavailable"
  fi
  echo "  telemetry.jsonl: $(wc -l < "$JSONL_FILE" 2>/dev/null || echo "0") lines"
  if [[ -f "$DB_FILE" ]]; then
    local count
    count=$(sqlite3 "$DB_FILE" "SELECT COUNT(*) FROM telemetry;" 2>/dev/null) || count="error"
    echo "  telemetry.db: $count records"
  else
    echo "  telemetry.db: not created yet (run: fmetrics import)"
  fi
}

install_hints() {
  cat <<'EOF'
fmetrics dependencies:

  Required:
    sqlite3    sudo apt install sqlite3      # Debian/Ubuntu
               brew install sqlite           # macOS

  Optional (for predictions):
    python3    sudo apt install python3      # Debian/Ubuntu
               brew install python3          # macOS
EOF
}

# -------------------------
# Argument parsing
# -------------------------
SUBCOMMAND=""
OUTPUT="$DEFAULT_OUTPUT"
LIMIT="$DEFAULT_LIMIT"
FILTER_TOOL=""
FILTER_PROJECT=""
PREDICT_PATH=""
CLEAN_DAYS="$DEFAULT_CLEAN_DAYS"
DRY_RUN=0

POSITIONAL=()
while [[ $# -gt 0 ]]; do
  case "$1" in
    -h|--help)    usage; exit 0 ;;
    --version)    echo "fmetrics $VERSION"; exit 0 ;;
    --self-check) self_check; exit 0 ;;
    --install-hints) install_hints; exit 0 ;;
    -o|--output)
      [[ -n "${2:-}" ]] || die "Missing value for $1"
      OUTPUT="$2"; shift 2 ;;
    --tool)
      [[ -n "${2:-}" ]] || die "Missing value for $1"
      FILTER_TOOL="$2"; shift 2 ;;
    --project)
      [[ -n "${2:-}" ]] || die "Missing value for $1"
      FILTER_PROJECT="$2"; shift 2 ;;
    --limit)
      [[ -n "${2:-}" ]] || die "Missing value for $1"
      LIMIT="$2"
      [[ "$LIMIT" =~ ^[0-9]+$ ]] || die "--limit must be a positive integer"
      shift 2 ;;
    --days)
      [[ -n "${2:-}" ]] || die "Missing value for $1"
      CLEAN_DAYS="$2"
      [[ "$CLEAN_DAYS" =~ ^[0-9]+$ ]] || die "--days must be a positive integer"
      shift 2 ;;
    --dry-run) DRY_RUN=1; shift ;;
    -*) die "Unknown option: $1" ;;
    *)  POSITIONAL+=("$1"); shift ;;
  esac
done

# Validate output
case "$OUTPUT" in
  pretty|json) ;;
  *) die "Invalid output: $OUTPUT (use pretty or json)" ;;
esac

# Extract subcommand
SUBCOMMAND="${POSITIONAL[0]:-}"
[[ -n "$SUBCOMMAND" ]] || { usage; exit 0; }

case "$SUBCOMMAND" in
  stats)   cmd_stats ;;
  history) cmd_history ;;
  predict)
    PREDICT_PATH="${POSITIONAL[1]:-}"
    [[ -n "$PREDICT_PATH" ]] || die "predict requires a path argument"
    PREDICT_TOOL="$FILTER_TOOL"
    cmd_predict ;;
  import)  cmd_import ;;
  clean)   cmd_clean ;;
  profile) cmd_profile ;;
  *)       die "Unknown subcommand: $SUBCOMMAND. Run: fmetrics --help" ;;
esac
